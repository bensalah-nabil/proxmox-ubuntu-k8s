# Default values for nms.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Override namespace for NMS instance manager chart resources.
# By default, the chart creates resources in the release namespace.
# This may not be desirable when using this chart as a dependency.
# namespace: "example"

# When openshift.enabled is set, chart will support the openshift deployment. Unsetting or keeping this as it is will
# lead to the kubernetes deployment by default.
# For example, to support openshift platform, helm install nim ./nim-charts -f values.yaml --set openshift.enabled=true
openshift:
  enabled: false

# Set admin password for NMS using --set flag with helm install
# For example, with openssl LibreSSL 2.8.3, helm install --set adminPasswordHash=$(openssl passwd -1 "YouPassword123#")
adminPasswordHash: "$6$D6DED2cNymdtFW1M$fUEYowW8FJk/aB1STmGQyO2GFqXfNJXbR2ZXkSgp3Tljp7s7VkwH3a2E1UpP/WG2nO1b8cvxC2w2nafUxPejv/"

# Use imagePullSecret to pass secret resources to pull all docker images from private registry.
# https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/:
imagePullSecrets:
  []
  # - name: "your-image-pull-secret"

nameOverride: "nms"
fullnameOverride: "nms"

commonLabels: {} # labels to add on all helm-managed resources
podLabels: {} # labels to add to all pods

# initialise global value for use by modules
global:
  utility: false
  nmsModules: {}

## nmsClickhouse server configuration packaged with helm
## Enable this if external clickhouse is disabled
nmsClickhouse:
  # If enabled, chart creates a clickhouse deployment required by NMS
  enabled: true
  fullnameOverride: clickhouse
  image:
    # public clickhouse published image
    # https://hub.docker.com/r/yandex/clickhouse-server
    repository: mirror.gcr.io/clickhouse/clickhouse-server
    # NMS recommended clickhouse server version
    tag: 24.8.12.28-alpine
    pullPolicy: IfNotPresent
  ## Authentication
  ## @param auth.username ClickHouse Admin username
  ## @param auth.password ClickHouse Admin password
  ## @param auth.existingSecret Name of a secret containing the Admin password
  ## @param auth.existingSecretKey Name of the key inside the existing secret
  ##
  auth:
    username: default
    password: ""
    existingSecret: ""
    existingSecretKey: ""
  ##
  ## Clickhouse service
  ##
  service:
    name: clickhouse
    rpcPort: 9000
    httpPort: 8123
    labels: {}
  ##
  ## Clickhouse server resources
  ##
  resources:
    requests:
      # These are minimum requirements on node to run clickhouse server
      # There are no limits in place, so vertically scale as per data needs
      cpu: 120m
      memory: 1Gi

  # When persistence.enabled is set, chart uses userVolumeMounts and userVolumes to persist data outside its pod.
  # A reference example is included in example-values for enterprise setup.
  persistence:
    enabled: true

    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If defined, PVC must be created manually before volume will be bound
    existingClaim:

    ## nms data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    ## Storage class of PV to bind. By default, it looks for standard storage class.
    ## If the PV uses a different storage class, specify that here.
    ## https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource
    storageClass: ""
    volumeName: ""
    accessMode: ReadWriteOnce
    size: 1Gi
  ## Tolerations for use with node taints
  tolerations: []
  pod:
    labels: {}
    # User ID to run clickhouse container as
    # 101 is the `clickhouse` user configured in https://hub.docker.com/r/clickhouse/clickhouse-server/ image
    uid: 101

## External clickhouse configuration provided by user (not necessarily running in kubernetes)
externalClickhouse:
  # Address of the external cluster. This is required when clickhouse.enabled is false
  address:
  ## Authentication
  ## @param auth.username ClickHouse Admin username
  ## @param auth.password ClickHouse Admin password
  ## @param auth.existingSecret Name of a secret containing the Admin password
  ## @param auth.existingSecretKey Name of the key inside the existing secret
  ##
  auth:
    username:
    password: ""
    existingSecret: ""
    existingSecretKey: ""

##
## NMS service account
##
serviceAccount:
  annotations: {}
  labels: {}

##
# All properties used by API-GW
##
apigw:
  name: apigw
  # To bring your own NGINX API Gateway certificates for hosting HTTPS NMS server,
  # set "tlsSecret" to an existing kubernetes secret name in the same namespace as the chart.
  # We recommend to set "tlsSecret" for production use case to manage certs.
  # By default, this helm chart creates it's own CA to self-sign HTTPS server cert key pair. These are not managed.
  # Follow our installation guide to get an example of a "tlsSecret" resource.
  tlsSecret: ""
  image:
    # if the image is on a private registry, provide imagePullSecrets
    # For using NGINX Plus, update the repository name to your NGINX Plus docker image
    repository: apigw
    tag: latest
    pullPolicy: IfNotPresent
  service:
    # Other supported values are NodePort, LoadBalancer.
    type: ClusterIP
    # Optional externalIPs to be used when service type is ClusterIP.
    externalIPs: []
    httpsPort: 443
    # When it's type NodePort, use nodePortHttps to set a static value.
    # If left empty, k8s will generate an ephemeral NodePort.
    nodePortHttps:
    labels: {}
  resources:
    requests:
      # These are minimum requirements on node to run NGINX api-gw
      # There are no limits in place, so vertically scale as per data needs
      cpu: 100m
      memory: 256Mi
  ## Tolerations for use with node taints
  tolerations: []
  pod:
    labels: {}

# All properties used by Core process
core:
  name: core
  image:
    # if the image is on a private registry, provide imagePullSecrets
    repository: core
    tag: latest
    pullPolicy: IfNotPresent
  container:
    port:
      http: 8033
      db: 7891
      grpc: 8038
  service:
    httpPort: 8033
    grpcPort: 8038
    labels: {}
  resources:
    requests:
      # These are minimum requirements on node to run core
      # There are no limits in place, so vertically scale as per data needs
      cpu: 10m
      memory: 512Mi

  # When persistence.enabled is set, chart uses userVolumeMounts and userVolumes to persist data outside its pod.
  # A reference example is included in example-values for enterprise setup.
  persistence:
    enabled: true

    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If existingClaim is defined, PVC must be created manually before volume will be bound
    claims:
      - name: dqlite
        existingClaim:
        size: 500Mi
        accessMode: ReadWriteOnce

      - name: secrets
        existingClaim:
        size: 128Mi
        accessMode: ReadWriteOnce

    ## nms data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    ## Storage class of PV to bind. By default, it looks for standard storage class.
    ## If the PV uses a different storage class, specify that here.
    ## https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource
    storageClass: ""
    volumeName: ""
  ## Tolerations for use with node taints
  tolerations: []
  pod:
    labels: {}

# All properties used by DPM process
dpm:
  name: dpm
  image:
    # if the image is on a private registry, provide imagePullSecrets
    repository: dpm
    tag: latest
    pullPolicy: IfNotPresent
  container:
    port:
      http: 8034
      grpc: 8036
      nats: 9100
      db: 7890
  service:
    httpPort: 8034
    grpcPort: 8036
    natsPort: 9100
    labels: {}
  resources:
    # These are minimum requirements on node to run dpm
    # There are no limits in place, so vertically scale as per data needs
    requests:
      cpu: 100m
      memory: 512Mi

  # When persistence.enabled is set, chart uses userVolumeMounts and userVolumes to persist data outside its pod.
  # A reference example is included in example-values for enterprise setup.
  persistence:
    enabled: true

    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If existingClaim is defined, PVC must be created manually before volume will be bound
    claims:
      - name: dqlite
        existingClaim:
        size: 1Gi
        accessMode: ReadWriteOnce

      - name: nats-streaming
        existingClaim:
        size: 1Gi
        accessMode: ReadWriteOnce

    ## nms data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    ## Storage class of PV to bind. By default, it looks for standard storage class.
    ## If the PV uses a different storage class, specify that here.
    ## https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource
    storageClass: ""
    volumeName: ""
  ## Tolerations for use with node taints
  tolerations: []
  pod:
    labels: {}

# All properties used by Ingestion process
ingestion:
  name: ingestion
  image:
    # if the image is on a private registry, provide imagePullSecrets
    repository: ingestion
    tag: latest
    pullPolicy: IfNotPresent
  # default replica count but can be increased as needed
  replicaCount: 1
  container:
    port:
      grpc: 8035
  service:
    grpcPort: 8035
    labels: {}
  resources:
    # These are minimum requirements on node to run ingestion
    # There are no limits in place, so vertically scale as per data needs
    requests:
      cpu: 100m
      memory: 512Mi
  ## Tolerations for use with node taints
  tolerations: []
  pod:
    labels: {}

secmon:
  name: secmon
  image:
    # if the image is on a private registry, provide imagePullSecrets
    repository: secmon
    tag: latest
    pullPolicy: IfNotPresent
  # default replica count but can be increased as needed
  replicaCount: 1
  container:
    port:
      http: 8039
  service:
    http: 8039
    labels: {}
  resources:
    # These are minimum requirements on node to run secmon
    # There are no limits in place, so vertically scale as per data needs
    requests:
      cpu: 100m
      memory: 512Mi
  ## Tolerations for use with node taints
  tolerations: []
  pod:
    labels: {}

# All properties used by Integrations process
integrations:
  name: integrations
  image:
    # if the image is on a private registry, provide imagePullSecrets
    repository: integrations
    tag: latest
    pullPolicy: IfNotPresent
  container:
    modeOfOperation: connected
    port:
      http: 8037
      db: 7892
      licenseDB: 7893
  service:
    httpPort: 8037
    labels: {}
  resources:
    # These are minimum requirements on node to run Integrations
    # There are no limits in place, so vertically scale as per data needs
    requests:
      cpu: 100m
      memory: 512Mi

  # When persistence.enabled is set, chart uses userVolumeMounts and userVolumes to persist data outside its pod.
  # A reference example is included in example-values for enterprise setup.
  persistence:
    enabled: true

    ## A manually managed Persistent Volume and Claim
    ## Requires persistence.enabled: true
    ## If existingClaim is defined, PVC must be created manually before volume will be bound
    claims:
      - name: dqlite
        existingClaim:
        size: 1Gi
        accessMode: ReadWriteOnce

      - name: proxy-certs
        existingClaim:
        size: 128Mi
        accessMode: ReadWriteOnce

    ## nms data Persistent Volume Storage Class
    ## If defined, storageClassName: <storageClass>
    ## If set to "-", storageClassName: "", which disables dynamic provisioning
    ## If undefined (the default) or set to null, no storageClassName spec is
    ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
    ##   GKE, AWS & OpenStack)
    ##
    ## Storage class of PV to bind. By default, it looks for standard storage class.
    ## If the PV uses a different storage class, specify that here.
    ## https://kubernetes.io/docs/concepts/storage/storage-classes/#the-storageclass-resource
    storageClass: ""
    volumeName: ""
  ## Tolerations for use with node taints
  tolerations: []
  pod:
    labels: {}

# All properties used by Utility process
utility:
  name: utility
  image:
    # if the image is on a private registry, provide imagePullSecrets
    repository: utility
    tag: latest
    pullPolicy: IfNotPresent

  ## Tolerations for use with node taints
  tolerations: []

# All properties used by Hashicorp Vault
nmsVault:
  # If enabled, chart creates a deployment of Hashicorp's Vault for development only
  enabled: false
  fullnameOverride: vault
  image:
    # public vault published image
    # https://hub.docker.com/_/vault
    repository: vault
    # NMS recommended vault server version
    tag: 1.12.2
    pullPolicy: IfNotPresent
  service:
    name: vault
    rpcPort: 8200
    labels: {}
  pod:
    labels: {}

# External vault configuration provided by user (e.g. address: http://ip-address:8200:v1 )
externalVault:
  address:

# Logging properties
log:
  # encoding type set for logging output, options = json, console (default)
  encoding: console
  # level set for logging output, options = debug, info, warn, error (default), panic, fatal
  level: error

# Network policies
networkPolicies:
  # If enabled, will create a network policies for NMS
  enabled: true

agent:
  # Enable mTLS between Agent and NMS NGINX API-GW.
  secure: false

proxyConfig:
  # Enable this setting to use proxy
  proxyEnable: false
  # Configure this setting to set the host of the proxy server to be used
  proxyHost: ""
  # Configure this setting to set the port of the proxy server to be used
  proxyPort: 3128
  # Configure this setting to set the protocol of the proxy server to be used - http/https (default - http)
  proxyProtocol: http
  # Enable this setting to use basic auth for proxy
  proxyAuthRequired: false
  # Configure this setting to set the user name in basic auth (proxy_auth_required: true)
  proxyUsername: ""
  # Configure this setting to set the password in basic auth (proxy_auth_required: true)
  proxyPassword: ""
  # Configure this setting to set the proxy connection verification (is the proxy trusted by the host)
  proxySslVerify: true

